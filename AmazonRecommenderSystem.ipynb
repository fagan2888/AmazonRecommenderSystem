{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Recommender System\n",
    "\n",
    "## Objective:\n",
    "\n",
    "- Using the supplied dataset build a a simple recommendation engine that given a user ID ( userId ) returns a collection (list) of recommended movies. \n",
    "- The recommendation engine can be written in any language (ruby, python, php, javascript, bash, etc) and built using any technology/framework but you are required to state why you chosen that language and the technology stack.\n",
    "- Bonus points who builds the solution integrated with a web interface. Please comment your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Plan:\n",
    "\n",
    "1. Data Collection and Preparation:\n",
    "2. Model Selection and Training\n",
    "    - Decide which algorithm to use\n",
    "    - Train algorithm and evaluate performance\n",
    "3. Make predictions\n",
    "4. Query Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook contain all the code generated to produce a final dictionary with the recommended movies for each user. This dictionary is saved in a pickle file.\n",
    "\n",
    "To simply get the movies' list for a specified user, one can simply use the query function on section 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_N_MOVIES = 10; # number of top movies to retrieve\n",
    "FILE_PATH = 'movies.txt' # path to the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Data Collection and Preparation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very important to filter the data that is indeed useful for our purpose. Our data per product contains the following information: \n",
    "1. productId\n",
    "2. userId\n",
    "3. profileName\n",
    "4. helpfulness\n",
    "5. score\n",
    "6. time\n",
    "7. summary\n",
    "8. review\n",
    "\n",
    "For our purpose, we can simply filter the productID, userID and score. This information will be enough to recommender a movie based on the tecnhiques we plan to use later on. Let's proceed to read the `.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(file_name, df, n_products):\n",
    "    counter = 0\n",
    "    with open(file_name, errors = 'ignore') as openfileobject:\n",
    "        for line in openfileobject:\n",
    "            counter = counter + 1\n",
    "            if counter == 500000:\n",
    "                break\n",
    "            if 'product/productId:' in line:\n",
    "                new_product = {'productId': line.split(':')[1].strip()}\n",
    "            elif 'review/userId:' in line:\n",
    "                new_product['userId'] = line.split(':')[1].strip()\n",
    "            elif 'review/score:' in line:\n",
    "                new_product['score'] = line.split(':')[1].strip()\n",
    "                df = df.append(new_product, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pandas dataframe to store reviews\n",
    "reviews = pd.DataFrame(columns=['productId', 'userId', 'score'])\n",
    "\n",
    "# read data file and append info to our dataframe\n",
    "reviews = read_dataset(FILE_PATH, reviews, 500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Model Selection and Training:\n",
    "\n",
    "In this part, we will select some models from the library [Surprise](http://surpriselib.com/), a full-fledged python library, specialized for recommender systems. This library will save much of the work that would be required if we were to implement the algorthms from scratch.\n",
    "\n",
    "After the famous [Netflix Prize Competition](https://en.wikipedia.org/wiki/Netflix_Prize) the Matrix Factorization proved to be one of the most efficient algorithms for recommender systems, performing better than the so far very commonly used collaborative filtering algorithms (user and item based). For this reason, we will use this implementation for our own recommender system.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import evaluate, print_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformData(df):\n",
    "    '''\n",
    "    Function to read data from dataframe\n",
    "    into Surprise data format\n",
    "    '''\n",
    "    df = df[['userId', 'productId', 'score']] # order imposed by surprise\n",
    "\n",
    "    reader = Reader(rating_scale=(1, 5))\n",
    "    data = Dataset.load_from_df(df[['userId', 'productId', 'score']], reader)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def trainAlgo(data, algorithm):\n",
    "    '''\n",
    "    Function to train requested algorithm\n",
    "    on full dataset\n",
    "    '''\n",
    "    data.split(n_folds=3)\n",
    "    algo = algorithm\n",
    "\n",
    "    # Evaluate performances of our algorithm on the dataset.\n",
    "    perf = evaluate(algo, data, measures=['MAE'], verbose = 0)\n",
    "\n",
    "    print_perf(perf)\n",
    "\n",
    "    return algo, data.build_full_trainset()\n",
    "\n",
    "\n",
    "def get_top_n(n, trainset, algo):\n",
    "    '''\n",
    "    Function to return a dictionary with top elements\n",
    "    per user\n",
    "    '''\n",
    "    testset = trainset.build_anti_testset()\n",
    "    predictions = algo.test(testset)\n",
    "    \n",
    "    # Build dictionary per user.\n",
    "    # Each user contains a list of tuples (itemId, score)\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "        \n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Fold 1  Fold 2  Fold 3  Mean    \n",
      "MAE     0.8689  0.8557  0.8655  0.8634  \n"
     ]
    }
   ],
   "source": [
    "# 1. get data into Surprise format\n",
    "data = transformData(reviews)\n",
    "\n",
    "# 2. traing algorithm, defined as the second parameter\n",
    "algo, trainset = trainAlgo(data, SVD())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Make Predictions\n",
    "\n",
    "We proceed to make predictions with the algorithm trained above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. get a dictionary with all predictions\n",
    "top_n = get_top_n(TOP_N_MOVIES, trainset, algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( top_n, open( \"recommendations.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def searchMovies(user, recommendations):\n",
    "    print('User {} would be delighted to watch the following movies:\\n'.format(user))\n",
    "    return recommendations[user]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User A141HP4LYPWMSR would be delighted to watch the following movies:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('B005LAJ22Q', 5),\n",
       " ('B00008WJDK', 4.835127692328798),\n",
       " ('B00004WCM9', 4.8065915057153745),\n",
       " ('B002UIGMYS', 4.787991941890827),\n",
       " ('B000063W82', 4.785120125760392),\n",
       " ('B0087ZG7RK', 4.776743153138881),\n",
       " ('B00004RFIE', 4.742585255506958),\n",
       " ('6300147967', 4.737080058199667),\n",
       " ('B000O599VC', 4.732156899585864),\n",
       " ('B000T4SWXO', 4.729831824023032)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TYPE USER\n",
    "USER = 'A141HP4LYPWMSR' # user to look for\n",
    "\n",
    "# load dictionary from pickle file\n",
    "recommendations = pickle.load( open( \"recommendations.p\", \"rb\" ) )\n",
    "\n",
    "searchMovies(USER, recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "**Some interestings points worth mentioning:**\n",
    "\n",
    "- For the purpose of this exercise, there was not much worry about evaluating the performance of different algorithms, nor tuning parameters. This could be important if we want to achieve a better performance. \n",
    "- Furthermore, the more data we consider the better our model will perform."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
